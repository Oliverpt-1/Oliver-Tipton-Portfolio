{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67a5df-5e9d-4abf-a928-b70d9059dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.5f}\".format(x)})\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea2865-a7eb-4db1-9c34-f6d2f5168953",
   "metadata": {},
   "outputs": [],
   "source": [
    "games = np.load(\"games.npz\")\n",
    "for game_name in sorted(games.keys()):\n",
    "    G = games[game_name]\n",
    "    P = G.shape[-1]\n",
    "    A = G.shape[:-1]\n",
    "    print(game_name + \":\", P, \"players,\", A, \"actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e10ca-5da1-458b-a3ed-140047e19b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken = games[\"chicken\"]\n",
    "pk3 = games[\"pk_3_actions\"]\n",
    "ttf = games[\"two_by_three_by_four\"]\n",
    "rps = games[\"rock_paper_scissors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745db3ba-ac74-4dd2-b5a7-ba46b34d514f",
   "metadata": {},
   "source": [
    "To play nice with JAX, we need to represent a profiles as a 2D array instead of as a list of 1D arrays.\n",
    "We accomplish this by padding out the actions dimension to the maximum number of actions available to any player, filling in zeros for the probability of invalid actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ea471-c7f3-4d55-a42e-660d5362deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_profile(game):\n",
    "    num_players = game.shape[-1]\n",
    "    max_actions = max(game.shape[:-1])\n",
    "    profile = np.zeros([num_players, max_actions])\n",
    "    for p in range(num_players):\n",
    "        num_actions = game.shape[p]\n",
    "        profile[p,:num_actions] = np.ones(num_actions) / num_actions\n",
    "    return profile\n",
    "\n",
    "def random_profile(game):\n",
    "    num_players = game.shape[-1]\n",
    "    max_actions = max(game.shape[:-1])\n",
    "    profile = np.zeros([num_players, max_actions])\n",
    "    for p in range(num_players):\n",
    "        num_actions = game.shape[p]\n",
    "        profile[p,:num_actions] = np.random.dirichlet([1]*num_actions)\n",
    "    return profile\n",
    "\n",
    "print(uniform_profile(ttf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b050a1-8594-46a0-ab52-2d118cf975cc",
   "metadata": {},
   "source": [
    "This requires a re-write of the `deviation_payoffs` function to slice out the appropriate sub-arrays.\n",
    "Rather than making you do this yourself, I'm providing a working implementation that's vectorized and that should also play nice with JAX in other ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff889b-51a4-46b0-a0ff-b53b4d46e59f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deviation_payoffs(game, profile, player):\n",
    "    num_players = game.shape[-1]\n",
    "    pay\n",
    "    offs = game[...,player]\n",
    "    for p in range(num_players):\n",
    "        if p != player:\n",
    "            a = game.shape[p]\n",
    "            payoffs = jnp.swapaxes(jnp.swapaxes(payoffs, p, -1) * profile[p,:a], p, -1)\n",
    "    payoffs = jnp.swapaxes(payoffs, player, -1)\n",
    "    return jnp.apply_over_axes(jnp.sum, payoffs, range(num_players-1))\n",
    "\n",
    "print(deviation_payoffs(chicken, uniform_profile(chicken), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4bca5d",
   "metadata": {},
   "source": [
    "Helper functions for normalizing/projecting onto a probability simplex. Both functions take a vector and project it onto the probability simplex of the same dimension. `simplex_normalize` assumes all entries in the array are non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4284160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplex_normalize(array):\n",
    "    return array / np.sum(array)\n",
    "\n",
    "_SIMPLEX_BIG = 1 / np.finfo(float).resolution\n",
    "def simplex_project(array):\n",
    "    \"\"\"Return the projection onto the simplex\"\"\"\n",
    "    array = np.asarray(array, float)\n",
    "    # check(not np.isnan(array).any(), \"can't project nan onto simplex: {}\", array)\n",
    "    # This fails for really large values, so we normalize the array so the\n",
    "    # largest element has absolute value at most _SIMPLEX_BIG\n",
    "    array = np.clip(array, -_SIMPLEX_BIG, _SIMPLEX_BIG)\n",
    "    size = array.shape[-1]\n",
    "    sort = -np.sort(-array, -1)\n",
    "    rho = (1 - sort.cumsum(-1)) / np.arange(1, size + 1)\n",
    "    inds = size - 1 - np.argmax((rho + sort > 0)[..., ::-1], -1)\n",
    "    rho.shape = (-1, size)\n",
    "    lam = rho[np.arange(rho.shape[0]), inds.flat]\n",
    "    lam.shape = array.shape[:-1] + (1,)\n",
    "    return np.maximum(array + lam, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527020d8-a9d8-4505-ba0d-704ba308fd54",
   "metadata": {},
   "source": [
    "The `deviation_gains` function returns an array with the deviation gain for each of the player's actions; an efficient implementation should only call `deviation_payoffs` once, and should use vectorized operations to avoid loops. The `total_gain` function returns the sum of the the action-gains for each player & action; `deviation_gains` should be called once per player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1b776-5ef3-43d0-97e8-aae3de48669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deviation_gains(game, profile, player):\n",
    "    deviation = deviation_payoffs(game, profile, player)\n",
    "    expected_utility = jnp.dot(deviation, profile[player])\n",
    "    gain = jnp.maximum(0, deviation - expected_utility)\n",
    "    return gain\n",
    "\n",
    "def total_gain(game, profile):\n",
    "    num_players = game.shape[-1]\n",
    "    gain = 0\n",
    "    for p in range(num_players):\n",
    "        deviation = deviation_payoffs(game,profile,p)\n",
    "        expected_util = jnp.dot(deviation,profile[p])\n",
    "        for i in range(len(deviation[0])):\n",
    "            gain += jnp.sum(jnp.maximum(0,deviation[i]-expected_util[0]))\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351abeba",
   "metadata": {},
   "source": [
    "You should expand the following set of tests for the gain functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65693081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(w):\n",
    "    return w / jnp.sum(w, axis=1).reshape(w.shape[0],1)\n",
    "\n",
    "def regret_matching(game, iterations=200, initial_profile=None, initial_weight=1):\n",
    "    if initial_profile is None:\n",
    "        initial_profile = uniform_profile(game)\n",
    "    gains = [mixed_strat*initial_weight for mixed_strat in initial_profile]\n",
    "    for i in range(iterations):\n",
    "        for p in range(game.shape[-1]):\n",
    "            deviation = deviation_gains(game, initial_profile, p)\n",
    "            gains[p] = gains[p] + deviation \n",
    "        profile = [normalize(g) for g in gains]   \n",
    "    return profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb915334-1159-460e-9ca5-d6991db5f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplotope_project(game, profile):\n",
    "    projected = np.zeros_like(profile)\n",
    "    for player in range(profile.shape[0]):\n",
    "        num_Actions = game.shape[player]\n",
    "        mix = profile[player, :num_Actions]\n",
    "        projected[player, :num_Actions] = simplex_project(mix)\n",
    "    return projected\n",
    "\n",
    "def gradient_descent(game, iterations=200, initial_profile=None, step_size=0.01):\n",
    "    num_players = game.shape[-1]\n",
    "    if initial_profile is None:\n",
    "        initial_profile = uniform_profile(game)\n",
    "    gain_gradient = jax.grad(lambda prof: total_gain(game, prof))\n",
    "    curr_profile = initial_profile\n",
    "    for i in range(iterations):\n",
    "        grad = gain_gradient(curr_profile)\n",
    "        curr_profile = curr_profile - (step_size*grad)\n",
    "        curr_profile = simplotope_project(game, curr_profile)\n",
    "    return curr_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicator_dynamics(game, iterations=200, initial_profile=None, min_payoffs=None):\n",
    "    num_players = game.shape[-1]\n",
    "    if initial_profile is None:\n",
    "        initial_profile = uniform_profile(game)\n",
    "    if min_payoffs is None:\n",
    "        min_payoffs = game.min(axis=tuple(range(num_players)))\n",
    "    \n",
    "    curr_profile = initial_profile\n",
    "    for i in range(iterations):\n",
    "        new_profile = np.zeros_like(initial_profile)\n",
    "        for p in range(game.shape[-1]):\n",
    "            dev_pays = deviation_payoffs(game, curr_profile, p)\n",
    "            dev_pays -= min_payoffs[p]\n",
    "            new_profile[p] = dev_pays * curr_profile[p]\n",
    "        curr_profile = normalize(new_profile)    \n",
    "                \n",
    "    return curr_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c654d2",
   "metadata": {},
   "source": [
    "You have implemented the following helper functions on previous assignments; feel free to copy over existing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a3796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regret(game, profile, player):\n",
    "    strat_util = 0\n",
    "    utilities = deviation_payoffs(game, profile, player)\n",
    "    utilities = utilities[0]\n",
    "    highest = -float(\"inf\")\n",
    "    for a in range(len(profile[player])):\n",
    "        strat_util += (utilities[a] * profile[player][a])\n",
    "        if (utilities[a] > highest):\n",
    "            highest = utilities[a]\n",
    "    regret = highest - strat_util\n",
    "    return regret\n",
    "\n",
    "\n",
    "def is_epsilon_equilibrium(game, mixed_profile, epsilon):\n",
    "    list_regret = []\n",
    "    for p in range(len(mixed_profile)):\n",
    "        list_regret.append(regret(game, mixed_profile, p))\n",
    "    highest = max(list_regret)\n",
    "    return epsilon >= highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464e461-1fd3-4610-a641-542ad42ee6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_regrets(game, candidate_equilibria, epsilon=1e-4):\n",
    "    list_equilibria = []\n",
    "    for p in range(len(candidate_equilibria)):\n",
    "        if(is_epsilon_equilibrium(game, candidate_equilibria[p], epsilon)):\n",
    "            list_equilibria.append(candidate_equilibria[p])\n",
    "    return list_equilibria\n",
    "\n",
    "def filter_unique(candidate_equilibria, min_dist=1e-2):\n",
    "    size = len(candidate_equilibria)\n",
    "    sorted_list = []\n",
    "    unique_equilibria = []\n",
    "    unique_equilibria = candidate_equilibria[0]\n",
    "    for i in range(size, 1, 1):\n",
    "        for u in range(len(unique_equilibria)):\n",
    "            if(np.allclose(unique_equilibria[u], candidate_equilibria[i])):\n",
    "                unique_equilibria.append(candidate_equilibria[i], atol = min_dist)\n",
    "    return unique_equilibria\n",
    "\n",
    "\n",
    "def Nash_local_search(game, method=gradient_descent, restarts=2, **search_kwds):\n",
    "    candidate = []\n",
    "    for i in range(restarts):\n",
    "        prof = random_profile(game)\n",
    "        candidate.append(method(game, initial_profile = prof, **search_kwds))\n",
    "    print(candidate)\n",
    "    candidate = filter_regrets(game, candidate)\n",
    "    print(candidate)\n",
    "    candidate = filter_unique(game, candidate)\n",
    "    return candidate\n",
    "\n",
    "print(Nash_local_search(chicken, method = gradient_descent, step_size =0.001, iterations = 200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0552c3c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Test `Nash_local_search` with all three algorithms (RM, RD, GD) on several different games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2174c53-cc0f-4250-8efa-b6f7db998988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7",
   "language": "python",
   "name": "python-2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
