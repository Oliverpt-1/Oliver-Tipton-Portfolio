{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.4f}\".format(x)})\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "from itertools import combinations_with_replacement, product, chain\n",
    "from collections import Counter\n",
    "from scipy.special import loggamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the multinomial: sum(counts) choose c_1, c_2, ...\n",
    "def multinomial(*counts):\n",
    "    return int(round(np.exp(loggamma(sum(counts) + 1) - sum(loggamma(np.array(counts) + 1)))))\n",
    "\n",
    "# computes n choose k\n",
    "def binomial(n,k):\n",
    "    return multinomial(k,n-k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following action-graph game representation stores two numpy arrays for each action: a table of all opponent-configurations over the neighborhood `self.config_tables[action]`, and a table of the action's payoff in each of these configurations `self.payoff_tables[action]`. Configurations are represented by a vector of counts for the number of players choosing each action in the neighborhood (the out-of-neighborhood count is implicit). The ordering of the configuration vector corresponds to the ordering of the neighboring actions in `self.action_graph[action]`. The ordering of the configuration and payoff tables match.\n",
    "\n",
    "Consider the 15-player lemonade-stand game below. Note that this lemonade-game variant has fewer edges than the one from the vieo: each action's neighborhood includes both adjacent locations for players of the same type, but only the matching location for players of the other type. The diagonal edges from the video are not present. This means, for example, that action *L3* has neighborhood *L2, L3, L4, H3*.\n",
    "\n",
    "The configuration table for action *L3* has shape (1320, 4), where the number of configurations corresponds to the `1320 = stars_and_bars(9, 4 - 1) * stars_and_bars(5, 2 - 1)` arrangements of 9 lemonade opponents and 5 hot-dog opponents over the neighborhood. Each configuration is a four-vector giving the number of opponents choosing each of *(L1, L2, L3, H2)*. At index 1009 of the configuration table, the configuration vector is `[0 7 2 5]`, and at index 1009 of the payoff table is the number `3.9994`. This means that when the 14 opponents jointly play `<0xL1, 7xL2, 2xL3, 2xH2>`, with 0 lemonade and 3 hot dog players choosing actions outside the neighborhood, a player selecting *L3* will get a payoff of `3.9994`.\n",
    "\n",
    "Your first task is to implement a `deviation_payoffs()` method for this `ActionGraphGame` class. The method takes as input a role-symmetric mixed-strategy profile (where each row gives the mixed strategy for one role) and outputs a deviation payoff for a player of the specified role using the specified action. To assist with the deviation payoff calculation, you will implement a helper function to compute the probability of a configuration under a given role-symmetric mixed-strategy profile, which in turn has a helper function to compute the number of asymmetric repetitions of a role-symmetric configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionGraphGame:\n",
    "    def __init__(self, json_data, payoff_function):\n",
    "        self.roles = [role for role in json_data[\"roles\"]]\n",
    "        self.num_roles = len(self.roles)\n",
    "        self.num_players = {role:num_players for role,num_players in json_data[\"players\"].items()}\n",
    "        self.action_sets = {role:actions for role,actions in json_data[\"actions\"].items()}\n",
    "        self.num_actions = {role:len(actions) for role,actions in json_data[\"actions\"].items()}\n",
    "        self.action_graph = {action:neighborhood for action,neighborhood in json_data[\"action_graph\"].items()}\n",
    "        self.total_actions = len(self.action_graph)\n",
    "        self.action_roles = {action:[role for role in self.roles if action in self.action_sets[role]][0] for action in self.action_graph} # assumes roles don't share actions\n",
    "        self.action_indices = {role:{action:index for index,action in enumerate(self.action_sets[role])} for role in self.roles}\n",
    "        self.role_config_indices = {action:{role:self._get_role_config_indices(action,role) for role in self.roles} for action in self.action_graph}\n",
    "        self.neighbor_masks = {action:self._get_neighbor_prof_mask(action) for action in self.action_graph}\n",
    "        \n",
    "        self.config_tables = {}\n",
    "        self.payoff_tables = {}\n",
    "        for action,neighbors in self.action_graph.items():\n",
    "            role_configs = []\n",
    "            for role in self.roles:\n",
    "                role_actions = [a for a in self.action_sets[role] if a in neighbors] + [None]\n",
    "                role_opponents = self.num_opponents(action, role)\n",
    "                role_configs.append(combinations_with_replacement(role_actions, role_opponents))\n",
    "            configs = product(*role_configs)\n",
    "            role_symmetric_configs = []\n",
    "            payoffs = []\n",
    "            for config in configs:\n",
    "                counts = Counter(chain(*config)) \n",
    "                vector_config = [counts[n] for n in neighbors]\n",
    "                dict_config = {n:counts[n] for n in neighbors}\n",
    "                role_symmetric_configs.append(vector_config)\n",
    "                payoffs.append(payoff_function(action, dict_config))\n",
    "            self.config_tables[action] = np.array(role_symmetric_configs)\n",
    "            self.payoff_tables[action] = np.array(payoffs)\n",
    "\n",
    "    def _get_role_neighborhood(self, action, role):\n",
    "        \"\"\"Determines which nodes in the neighborhood of action belong to role.\"\"\"\n",
    "        neighborhood = set(self.action_graph[action])\n",
    "        role_actions = set(self.action_sets[role])\n",
    "        return sorted(neighborhood & role_actions)\n",
    "    \n",
    "    def regret(self, sym_prof):\n",
    "        return jnp.max(self.deviation_gains(sym_prof))\n",
    "    \n",
    "    def _get_role_config_indices(self, action, role):\n",
    "        \"\"\"Determines which indices the action's configurations belong to role.\"\"\"\n",
    "        neighborhood = self.action_graph[action]\n",
    "        role_actions = self._get_role_neighborhood(action, role)\n",
    "        return [neighborhood.index(act) for act in role_actions]\n",
    "\n",
    "    def _get_neighbor_prof_mask(self, action):\n",
    "        \"\"\"Generates a 0,1 mask for the entries in a profile that belong to an action's neighborhood.\"\"\"\n",
    "        mask = np.zeros([self.num_roles, max(self.num_actions.values())], dtype=bool)\n",
    "        for r,role in enumerate(self.roles):\n",
    "            for n,node in enumerate(self.action_sets[role]):\n",
    "                if node in self.action_graph[action]:\n",
    "                    mask[r,n] = True\n",
    "        return mask\n",
    "\n",
    "    def num_opponents(self, action, role):\n",
    "        \"\"\"Determines the number of opponents playing role when the deviating player plays action.\"\"\"\n",
    "        player_role = self.action_roles[action]\n",
    "        if role == player_role:\n",
    "            return self.num_players[role] - 1\n",
    "        return self.num_players[role]\n",
    "    \n",
    "    def uniform_profile(self):\n",
    "        prof = np.zeros([self.num_roles, max(self.num_actions.values())])\n",
    "        for r,role in enumerate(self.roles):\n",
    "            prof[r] = np.ones(self.num_actions[role])/self.num_actions[role]\n",
    "        return prof\n",
    "    \n",
    "    def random_profile(self):\n",
    "        prof = np.zeros([self.num_roles, max(self.num_actions.values())])\n",
    "        for r,role in enumerate(self.roles):\n",
    "            prof[r] = np.random.dirichlet(np.ones(self.num_actions[role]))\n",
    "        return prof\n",
    "\n",
    "    def deviation_gains(self, role_sym_prof):\n",
    "        gain = np.zeros([self.num_roles, max(self.num_actions.values())])\n",
    "        for r, role in enumerate(self.roles):\n",
    "            deviation = self.deviation_payoffs(role_sym_prof, role)\n",
    "            expected_utility = jnp.dot(deviation, role_sym_prof[r])\n",
    "            gain[r] = jnp.maximum(0, deviation - expected_utility)\n",
    "        return gain\n",
    "\n",
    "    def deviation_payoffs(self, role_sym_prof, role):\n",
    "        arr = np.zeros(self.num_actions[role])\n",
    "        for a,act in enumerate(self.action_sets[role]):\n",
    "            arr[a] = self.deviation_payoff(role_sym_prof, role, act)\n",
    "        return arr\n",
    "        \n",
    "    def deviation_payoff(self, role_sym_prof, role, action):\n",
    "        sum = 0\n",
    "        for config, payoff in zip(self.config_tables[action], self.payoff_tables[action]):\n",
    "            sum += self.config_prob(role_sym_prof, config, action)*payoff\n",
    "        return sum\n",
    "\n",
    "    def config_prob(self, role_sym_prof, opp_config, action):\n",
    "        prob = 1\n",
    "        mask = self.neighbor_masks[action]\n",
    "        for r,role in enumerate(self.roles):\n",
    "            ind = self.role_config_indices[action][role]\n",
    "            role_prof = role_sym_prof[r]\n",
    "            role_mask = mask[r]\n",
    "            prob_neighborhood = role_prof[role_mask]\n",
    "            outside_prob = 1 - sum(prob_neighborhood)\n",
    "            num_opp = self.num_opponents(action, role)\n",
    "            role_config = opp_config[ind]\n",
    "            outside_opp = self.num_opponents(action, role) - sum(role_config)\n",
    "            exp = opp_config[self.role_config_indices[action][role]]\n",
    "            prob *= np.prod(prob_neighborhood ** exp)\n",
    "            prob *= (outside_prob ** outside_opp)\n",
    "        prob *= self.repetitions(opp_config, action)\n",
    "        return prob\n",
    "    \n",
    "    def repetitions(self, opp_config, action):\n",
    "        reps = 1\n",
    "        for r,role in enumerate(self.roles):\n",
    "            ind = self.role_config_indices[action][role]\n",
    "            role_config = opp_config[ind]\n",
    "            reps *= multinomial(*role_config, self.num_opponents(action, role) - sum(role_config))\n",
    "        return reps\n",
    "      #for each role there is a multinomial configuration for each role and oplayers outside of the neighborhood for each role\n",
    "      #for probabilities find probabiliteis in neighborhood to the count ^count\n",
    "      #Roles are basically lemonade stands or hot dogs for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``ActionGraphGame.__init__`` method takes as input a dictionary in JSON-serializable format with the following information:\n",
    "* names for each of the game's roles\n",
    "* the number of players for each role\n",
    "* the set of actions for each role\n",
    "* the action graph, represented by a mapping of actions to neighbors\n",
    "\n",
    "It also takes a payoff function which can generate a payoff for any configuration. This function takes as input and action and a dictionary representation of the neighborhood's opponent-configuration, for example ``\"L2\", {\"L1\":0, \"L2\":8, \"L3\":1, \"H2\":2}`` would ask for the payoff to action *L2* when the neighborhood configuration is ``<0xL1, 8xL2, 1xL3, 2xH2>``, with 0 lemonade and 3 hot dog players choosing actions outside the neighborhood. \n",
    "\n",
    "Moderately-configurable examples for the lemonade-stand game appear below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEMONADE_PLAYERS = 10\n",
    "HOT_DOG_PLAYERS = 5\n",
    "LOCATIONS = 8\n",
    "\n",
    "lemonade_settings = {\n",
    "  \"roles\":[\"lemonade\", \"hot_dogs\"], \n",
    "  \"players\":{\"lemonade\":LEMONADE_PLAYERS, \"hot_dogs\":HOT_DOG_PLAYERS},\n",
    "  \"actions\":{\"lemonade\":[\"L\"+str(i) for i in range(LOCATIONS)],\n",
    "             \"hot_dogs\":[\"H\"+str(i) for i in range(LOCATIONS)]},\n",
    "  \"action_graph\":{**{\"L\"+str(i):[\"L\"+str(j) for j in range(i-1,i+2) if j>=0 and j<=7]+[\"H\"+str(i)] for i in range(LOCATIONS)},\n",
    "                  **{\"H\"+str(i):[\"H\"+str(j) for j in range(i-1,i+2) if j>=0 and j<=7]+[\"L\"+str(i)] for i in range(LOCATIONS)}}\n",
    "}\n",
    "\n",
    "LEMONADE_MULTIPLIER = 2.0\n",
    "HOT_DOG_BONUS = 5.0\n",
    "NEIGHBOR_DISCOUNT = 0.9\n",
    "\n",
    "def lemonade_payoffs(action, opp_config):\n",
    "    role = action[0]\n",
    "    location = int(action[1])\n",
    "    payoff = (LOCATIONS+2)*2 - ((LOCATIONS-1)/2 - location)**2 # center of the beach is more popular\n",
    "    if role == \"L\" and opp_config[\"H\"+str(location)] > 0:\n",
    "        payoff *= LEMONADE_MULTIPLIER # scale the lemonade stand payoff if there's a hot dog stand at the same location\n",
    "    if role == \"H\":\n",
    "        payoff += HOT_DOG_BONUS * opp_config[\"L\"+str(location)] # hot dog stand gets a bonus for each co-located lemonade stand\n",
    "    payoff /= opp_config[action] + 1 # competitors at the same location split the payoff\n",
    "    if location > 0:\n",
    "        payoff *= NEIGHBOR_DISCOUNT**opp_config[role + str(location-1)] # nearby competitors also reduce payoff \n",
    "    if location < LOCATIONS-1:\n",
    "        payoff *= NEIGHBOR_DISCOUNT**opp_config[role + str(location+1)] # nearby competitors also reduce payoff \n",
    "    return payoff\n",
    "\n",
    "lemonade_game = ActionGraphGame(lemonade_settings, lemonade_payoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H7 neighborhood: ['H6', 'H7', 'L7']\n",
      "H7 config-table shape: (165, 3) \n",
      "\n",
      "L3 neighborhood: ['L2', 'L3', 'L4', 'H3']\n",
      "L3 config-table shape: (1320, 4) \n",
      "\n",
      "[[0 8 1 1]\n",
      " [0 8 1 0]\n",
      " [0 8 0 5]\n",
      " [0 8 0 4]\n",
      " [0 8 0 3]\n",
      " [0 8 0 2]\n",
      " [0 8 0 1]\n",
      " [0 8 0 0]\n",
      " [0 7 2 5]] \n",
      "\n",
      "[3.9500 1.9750 4.3889 4.3889 4.3889 4.3889 4.3889 2.1944 3.9994] \n",
      "\n",
      "{'lemonade': [0, 1, 2], 'hot_dogs': [3]} \n",
      "\n",
      "[[False False  True  True  True False False False]\n",
      " [False False False  True False False False False]]\n"
     ]
    }
   ],
   "source": [
    "print(\"H7 neighborhood:\", lemonade_game.action_graph[\"H7\"])\n",
    "print(\"H7 config-table shape:\", lemonade_game.config_tables[\"H7\"].shape, \"\\n\")\n",
    "print(\"L3 neighborhood:\", lemonade_game.action_graph[\"L3\"])\n",
    "print(\"L3 config-table shape:\", lemonade_game.config_tables[\"L3\"].shape, \"\\n\")\n",
    "print(lemonade_game.config_tables[\"L3\"][1000:1009,:], \"\\n\")\n",
    "print(lemonade_game.payoff_tables[\"L3\"][1000:1009], \"\\n\")\n",
    "\n",
    "# New since the version from class:\n",
    "print(lemonade_game.role_config_indices[\"L3\"], \"\\n\")\n",
    "print(lemonade_game.neighbor_masks[\"L3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5 1 2]\n",
      "[3 5 1]\n",
      "[[0.0428 0.0872 0.0059 0.0365 0.2073 0.0888 0.2048 0.3267]\n",
      " [0.1179 0.2833 0.1102 0.0551 0.1147 0.1887 0.0391 0.0909]]\n",
      "[[0 0 1 1 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "[0.0059 0.0365 0.2073 0.0551]\n",
      "[0.0059 0.0365 0.2073]\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the new attributes:\n",
    "A = \"L3\"\n",
    "R = \"lemonade\"\n",
    "opp_config = lemonade_game.config_tables[A][345]\n",
    "print(opp_config)\n",
    "print(opp_config[lemonade_game.role_config_indices[A][R]])\n",
    "prof = lemonade_game.random_profile()\n",
    "print(prof)\n",
    "mask = lemonade_game.neighbor_masks[A]\n",
    "print(mask.astype(int))\n",
    "print(prof[mask])\n",
    "print(prof[0][mask[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0000, 0.0000, 1.8963, 3.2750, 3.2750, 1.8963, 0.0000, 0.0000],\n",
       "       [0.0000, 0.0000, 1.9631, 3.3628, 3.3628, 1.9631, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: test your deviation payoffs functions incrementally as you develop them!\n",
    "prof = lemonade_game.uniform_profile()\n",
    "lemonade_game.deviation_gains(prof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the default lemonade game, I get the following results for `deviation_payoffs` on the uniform profile:\n",
    "\n",
    "`hot_dogs: [10.3429, 13.9965, 16.7958, 18.1954, 18.1954, 16.7958, 13.9965, 10.3429]`\n",
    "\n",
    "`lemonade: [6.0297, 9.4787, 12.2362, 13.6149, 13.6149, 12.2362, 9.4787, 6.0297]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hot_dogs: [10.3429 13.9965 16.7958 18.1954 18.1954 16.7958 13.9965 10.3429]\n",
      "lemonade: [6.0297 9.4787 12.2362 13.6149 13.6149 12.2362 9.4787 6.0297]\n"
     ]
    }
   ],
   "source": [
    "prof = lemonade_game.uniform_profile()\n",
    "print(\"hot_dogs:\", lemonade_game.deviation_payoffs(prof, \"hot_dogs\"))\n",
    "print(\"lemonade:\", lemonade_game.deviation_payoffs(prof, \"lemonade\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should adapt your `Nash_local_search` methods from previous classes, modify them to work on AGGs, and compute the role-symmetric mixed-strategy Nash equilibria of the lemonade game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(w):\n",
    "    return w / jnp.sum(w)\n",
    "\n",
    "def regret_matching(sym_game, iterations=200, initial_mixture=None, initial_weight=1):\n",
    "    if initial_mixture is None:\n",
    "        initial_mixture = sym_game.uniform_profile()\n",
    "    gains = initial_mixture * initial_weight\n",
    "    profile = initial_mixture\n",
    "    for i in range(iterations):\n",
    "        deviation = sym_game.deviation_gains(profile)\n",
    "        gains = gains + deviation \n",
    "        profile = normalize(gains)  \n",
    "    return profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_epsilon_equilibrium(sym_game, sym_prof, epsilon=0.001):\n",
    "        return sym_game.regret(sym_prof) < epsilon\n",
    "    \n",
    "def filter_regrets(sym_game, candidate_equilibria, epsilon=1e-2):\n",
    "    list_equilibria = []\n",
    "    for p in range(len(candidate_equilibria)):\n",
    "        if(is_epsilon_equilibrium(sym_game, candidate_equilibria[p], epsilon)):\n",
    "            list_equilibria.append(candidate_equilibria[p])\n",
    "    return list_equilibria\n",
    "\n",
    "\n",
    "def filter_unique(candidate_equilibria, min_dist=1e-2):\n",
    "    size = len(candidate_equilibria)\n",
    "    if size == 0:\n",
    "        return candidate_equilibria\n",
    "    sorted_list = []\n",
    "    unique_equilibria = []\n",
    "    unique_equilibria = candidate_equilibria[0]\n",
    "    for i in range(size, 1, 1):\n",
    "        for u in range(len(unique_equilibria)):\n",
    "            if(np.allclose(unique_equilibria[u], candidate_equilibria[i])):\n",
    "                unique_equilibria.append(candidate_equilibria[i], atol = min_dist)\n",
    "    return unique_equilibria\n",
    "\n",
    "def Nash_local_search(sym_game, method=regret_matching, restarts=10, eps = 1e-3, **search_kwds):\n",
    "    candidate = []\n",
    "    for i in range(restarts):\n",
    "        prof = sym_game.random_profile()\n",
    "        candidate.append(method(sym_game, initial_mixture = prof, **search_kwds))\n",
    "    candidate = filter_regrets(sym_game, candidate)\n",
    "    candidate = filter_unique(candidate)\n",
    "    return candidate\n",
    "\n",
    "Nash_local_search(lemonade_game, regret_matching, restarts = 1, eps = 1e-3, iterations = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
