{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6003cca1-a7af-4f7e-a0cc-f6987e192f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.4f}\".format(x)})\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from scipy.special import loggamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8aa115-3ab2-4744-b585-c80b7605f96a",
   "metadata": {},
   "source": [
    "Since we're working with symmetric games, we'll be looking for symmetric equilibria, which means we can represent profiles by storing a single mixed strategy that will be used by all players. This simplifies the following functions for generating profiles, and will also require us to re-write some of our other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c5ea471-c7f3-4d55-a42e-660d5362deb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2860 0.0410 0.2303 0.4427]\n"
     ]
    }
   ],
   "source": [
    "def uniform_mixture(num_actions):\n",
    "    return np.ones(num_actions) / num_actions\n",
    "\n",
    "def random_mixture(num_actions):\n",
    "    return np.random.dirichlet([1]*num_actions)\n",
    "\n",
    "print(random_mixture(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c654d2",
   "metadata": {},
   "source": [
    "The following functions compute binomial or multinomial coefficients, using the loggamma function to avoid intermediate overflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9704c1e2-1dfb-4c75-8f2e-89d32852c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the multinomial: sum(counts) choose c_1, c_2, ...\n",
    "def multinomial(*counts):\n",
    "    return int(round(np.exp(loggamma(sum(counts) + 1) - sum(loggamma(np.array(counts) + 1)))))\n",
    "\n",
    "# computes n choose k\n",
    "def binomial(n,k):\n",
    "    return multinomial(k,n-k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393e51f-42b8-4ffe-ad11-697206387abe",
   "metadata": {},
   "source": [
    "The `SymGame` class represents a symmetric game by storing a configurations array, a payoffs array, and a repeats array. Since we're using multiple arrays, it makes sense to package them into an object that also provides other methods. In particular, we ask the game to compute its own `deviation_payoffs`, `deviation_gains`, `total_gain`, and `regret`, and to check a candidate equilibrium. Some of these are left for you to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4d67286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement as CwR\n",
    "\n",
    "class SymGame:\n",
    "    def __init__(self, num_players, num_actions, payoff_func):\n",
    "        self.num_players = num_players\n",
    "        self.num_actions = num_actions\n",
    "        num_configs = binomial(num_players + num_actions - 2, num_players - 1)\n",
    "        self.configs = np.zeros([num_actions, num_configs], dtype=int)\n",
    "        self.payoffs = np.zeros([num_actions, num_configs], dtype=float)\n",
    "        self.repeats = np.zeros(num_configs, dtype=int)\n",
    "        \n",
    "        for c,cfg in enumerate(CwR(range(num_actions), num_players - 1)):\n",
    "            for a in cfg:\n",
    "                self.configs[a,c] += 1\n",
    "            self.payoffs[:,c] = payoff_func(self.configs[:,c])\n",
    "            self.repeats[c] = multinomial(*self.configs[:,c])\n",
    "            \n",
    "    def deviation_payoffs(self, sym_prof):\n",
    "        exponent = np.reshape(sym_prof, [sym_prof.shape[0], 1]) ** self.configs\n",
    "        product = np.prod(exponent, axis = 0)\n",
    "        probs = self.repeats*product\n",
    "        weighted_pays = probs*self.payoffs\n",
    "        dev_pays = np.sum(weighted_pays, axis = 1)\n",
    "        return dev_pays\n",
    "    \n",
    "    def deviation_gains(self, sym_prof):\n",
    "        deviation = self.deviation_payoffs(sym_prof)\n",
    "        expected_utility = jnp.dot(deviation, sym_prof)\n",
    "        gain = jnp.maximum(0, deviation - expected_utility)\n",
    "        return gain\n",
    "\n",
    "    def total_gain(self, sym_prof):\n",
    "        return jnp.sum(self.deviation_gains(sym_prof))\n",
    "    \n",
    "    def regret(self, sym_prof):\n",
    "        return jnp.max(self.deviation_gains(sym_prof))\n",
    "\n",
    "    def is_epsilon_equilibrium(self, sym_prof, epsilon=0.001):\n",
    "        return self.regret(sym_prof) < epsilon\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return \"Symmetric Game: P=\"+str(self.num_players)+\", A=\"+str(self.num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc13ad-fdaa-4dcc-8be6-eab0f0945b37",
   "metadata": {},
   "source": [
    "This shows an example of how we can implement a payoff function and use it to create a game. This is the game from Appendix A in the paper and the examples in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9b2b5c3-3d16-4ddb-bee4-5347f8621866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symmetric Game: P=3, A=3\n",
      "[[2 1 1 0 0 0]\n",
      " [0 1 0 2 1 0]\n",
      " [0 0 1 0 1 2]]\n",
      "[[0.0000 -1.0000 -1.0000 1.0000 1.0000 1.0000]\n",
      " [2.0000 -2.0000 2.0000 0.0000 -2.0000 2.0000]\n",
      " [3.0000 3.0000 -3.0000 3.0000 -3.0000 0.0000]]\n",
      "[1 2 2 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "def payoffs_from_appendix(opp_config):\n",
    "    num_opponents = sum(opp_config)\n",
    "    payoffs = np.zeros(opp_config.shape)\n",
    "    for i,c in enumerate(opp_config):\n",
    "        if c == 0:\n",
    "            payoffs[i] = i+1\n",
    "        elif c < num_opponents:\n",
    "            payoffs[i] = -(i+1)\n",
    "    return payoffs\n",
    "\n",
    "game_from_appendix = SymGame(3, 3, payoffs_from_appendix)\n",
    "print(game_from_appendix)\n",
    "print(game_from_appendix.configs)\n",
    "print(game_from_appendix.payoffs)\n",
    "print(game_from_appendix.repeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ec76a-dde9-4bc3-bc6b-2fc91d03b8c1",
   "metadata": {},
   "source": [
    "Use this as a starting point for testing your `deviation_payoffs` and `deviation_gains` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fff889b-51a4-46b0-a0ff-b53b4d46e59f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symmetric Game: P=3, A=3\n",
      "profile: [0.1000 0.5000 0.4000]\n",
      "deviation payoffs: [0.6300 -0.5000 -0.3600]\n",
      "deviation gains: [0.9610 0.0000 0.0000]\n",
      "total gain: 0.96099997\n",
      "regret: 0.96099997\n",
      "equilibrium? False\n"
     ]
    }
   ],
   "source": [
    "sym_game = game_from_appendix\n",
    "# sym_prof = random_mixture(sym_game.num_actions)\n",
    "# sym_prof = uniform_mixture(sym_game.num_actions)\n",
    "sym_prof = np.array([.1, .5, .4])\n",
    "print(sym_game)\n",
    "print(\"profile:\", sym_prof)\n",
    "print(\"deviation payoffs:\", sym_game.deviation_payoffs(sym_prof))\n",
    "print(\"deviation gains:\", sym_game.deviation_gains(sym_prof))\n",
    "print(\"total gain:\", sym_game.total_gain(sym_prof))\n",
    "print(\"regret:\", sym_game.regret(sym_prof))\n",
    "print(\"equilibrium?\", sym_game.is_epsilon_equilibrium(sym_prof))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46829554-bf55-4a42-8a3d-1722a6fa29ad",
   "metadata": {},
   "source": [
    "These helper functions are for normalizing or projecting a vector onto a probability simplex. Both functions take a vector and project it onto the probability simplex of the same dimension. `simplex_normalize` assumes all entries in the array are non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4284160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplex_normalize(array):\n",
    "    return array / np.sum(array)\n",
    "\n",
    "_SIMPLEX_BIG = 1 / np.finfo(float).resolution\n",
    "def simplex_project(array):\n",
    "    \"\"\"Return the projection onto the simplex\"\"\"\n",
    "    array = np.asarray(array, float)\n",
    "    #check(not np.isnan(array).any(), \"can't project nan onto simplex: {}\", array)\n",
    "    # This fails for really large values, so we normalize the array so the\n",
    "    # largest element has absolute value at most _SIMPLEX_BIG\n",
    "    array = np.clip(array, -_SIMPLEX_BIG, _SIMPLEX_BIG)\n",
    "    size = array.shape[-1]\n",
    "    sort = -np.sort(-array, -1)\n",
    "    rho = (1 - sort.cumsum(-1)) / np.arange(1, size + 1)\n",
    "    inds = size - 1 - np.argmax((rho + sort > 0)[..., ::-1], -1)\n",
    "    rho.shape = (-1, size)\n",
    "    lam = rho[np.arange(rho.shape[0]), inds.flat]\n",
    "    lam.shape = array.shape[:-1] + (1,)\n",
    "    return np.maximum(array + lam, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c413d9e-47c8-46e9-9473-ac9de5923158",
   "metadata": {},
   "source": [
    "The following functions that you've implemented before will all need to be updated slightly to work with symmetric profiles and our `SymGame` data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65693081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.3333, 0.3333, 0.3333], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(w):\n",
    "    return w / jnp.sum(w)\n",
    "\n",
    "def regret_matching(sym_game, iterations=200, initial_mixture=None, initial_weight=1):\n",
    "    if initial_mixture is None:\n",
    "        initial_mixture = uniform_mixture(sym_game.num_actions)\n",
    "    gains = initial_mixture * initial_weight\n",
    "    profile = initial_mixture\n",
    "    for i in range(iterations):\n",
    "        deviation = sym_game.deviation_gains(profile)\n",
    "        gains = gains + deviation \n",
    "        profile = normalize(gains)  \n",
    "    return profile\n",
    "\n",
    "regret_matching(sym_game, iterations = 2000, initial_mixture = sym_prof )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "807c8844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.3333, 0.3333, 0.3333], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replicator_dynamics(sym_game, iterations=200, initial_mixture=None, min_payoff=None):\n",
    "    if initial_mixture is None:\n",
    "        initial_mixture = uniform_mixture(sym_game.num_actions)\n",
    "    if min_payoff is None:\n",
    "        min_payoff = sym_game.payoffs.min()\n",
    "    \n",
    "    curr_profile = initial_mixture\n",
    "    for i in range(iterations):\n",
    "        new_profile = np.zeros_like(initial_mixture)\n",
    "        dev_pays = sym_game.deviation_payoffs(curr_profile)\n",
    "        dev_pays -= min_payoff\n",
    "        new_profile = dev_pays * curr_profile\n",
    "        curr_profile = normalize(new_profile)        \n",
    "    return curr_profile\n",
    "\n",
    "replicator_dynamics(sym_game, iterations = 200, initial_mixture = sym_prof )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb915334-1159-460e-9ca5-d6991db5f7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3322, 0.3325, 0.3353])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(sym_game, iterations=200, initial_mixture=None, step_size=0.001):\n",
    "    if initial_mixture is None:\n",
    "        initial_mixture = uniform_mixture(sym_game.num_actions)\n",
    "    \n",
    "    gain_gradient = jax.grad(lambda prof: sym_game.total_gain(prof))\n",
    "    curr_profile = initial_mixture\n",
    "    for i in range(iterations):\n",
    "        grad = gain_gradient(curr_profile)\n",
    "        curr_profile = curr_profile - (step_size*grad)\n",
    "        curr_profile = simplex_project(curr_profile)\n",
    "    return curr_profile\n",
    "\n",
    "gradient_descent(sym_game, iterations = 1000, initial_mixture = sym_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9464e461-1fd3-4610-a641-542ad42ee6b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gradient_descent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                 unique_equilibria\u001b[38;5;241m.\u001b[39mappend(candidate_equilibria[i], atol \u001b[38;5;241m=\u001b[39m min_dist)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unique_equilibria\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mNash_local_search\u001b[39m(sym_game, method\u001b[38;5;241m=\u001b[39m\u001b[43mgradient_descent\u001b[49m, restarts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msearch_kwds):\n\u001b[1;32m     23\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(restarts):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gradient_descent' is not defined"
     ]
    }
   ],
   "source": [
    "def filter_regrets(sym_game, candidate_equilibria, epsilon=1e-2):\n",
    "    list_equilibria = []\n",
    "    for p in range(len(candidate_equilibria)):\n",
    "        if(sym_game.is_epsilon_equilibrium(candidate_equilibria[p], epsilon)):\n",
    "            list_equilibria.append(candidate_equilibria[p])\n",
    "    return list_equilibria\n",
    "\n",
    "\n",
    "def filter_unique(candidate_equilibria, min_dist=1e-2):\n",
    "    size = len(candidate_equilibria)\n",
    "    if size == 0:\n",
    "        return candidate_equilibria\n",
    "    sorted_list = []\n",
    "    unique_equilibria = []\n",
    "    unique_equilibria = candidate_equilibria[0]\n",
    "    for i in range(size, 1, 1):\n",
    "        for u in range(len(unique_equilibria)):\n",
    "            if(np.allclose(unique_equilibria[u], candidate_equilibria[i])):\n",
    "                unique_equilibria.append(candidate_equilibria[i], atol = min_dist)\n",
    "    return unique_equilibria\n",
    "\n",
    "def Nash_local_search(sym_game, method=gradient_descent, restarts=10, **search_kwds):\n",
    "    candidate = []\n",
    "    for i in range(restarts):\n",
    "        prof = random_mixture(sym_game.num_actions)\n",
    "        candidate.append(method(sym_game, initial_mixture = prof, **search_kwds))\n",
    "    candidate = filter_regrets(sym_game, candidate)\n",
    "    candidate = filter_unique(candidate)\n",
    "    return candidate\n",
    "\n",
    "Nash_local_search(sym_game, method = gradient_descent, iterations = 1000, step_size = .0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ddee6-161a-4af9-b798-c5b9344204ea",
   "metadata": {},
   "source": [
    "Test your implementations! Here's a start, but you should add more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b786359a-616d-4197-a2d6-e0a7232ad6b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Nash_local_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      3\u001b[0m sym_game \u001b[38;5;241m=\u001b[39m SymGame(P, A, \u001b[38;5;28;01mlambda\u001b[39;00m prof: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,prof\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m----> 4\u001b[0m rm_eq \u001b[38;5;241m=\u001b[39m \u001b[43mNash_local_search\u001b[49m(sym_game, regret_matching, \u001b[38;5;241m10\u001b[39m, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, initial_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m rd_eq \u001b[38;5;241m=\u001b[39m Nash_local_search(sym_game, replicator_dynamics, \u001b[38;5;241m10\u001b[39m, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, min_payoffs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m gd_eq \u001b[38;5;241m=\u001b[39m Nash_local_search(sym_game, gradient_descent, \u001b[38;5;241m10\u001b[39m, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Nash_local_search' is not defined"
     ]
    }
   ],
   "source": [
    "P = 6\n",
    "A = 4\n",
    "sym_game = SymGame(P, A, lambda prof: np.random.uniform(0,1,prof.shape))\n",
    "rm_eq = Nash_local_search(sym_game, regret_matching, 10, iterations=200, initial_weight=1)\n",
    "rd_eq = Nash_local_search(sym_game, replicator_dynamics, 10, iterations=200, min_payoffs=None)\n",
    "gd_eq = Nash_local_search(sym_game, gradient_descent, 10, iterations=200, step_size=0.01)\n",
    "print(\"regret matching found\", len(rm_eq), \"equilibria:\", rm_eq)\n",
    "print(\"replicator dynamics found\", len(rm_eq), \"equilibria:\", rd_eq)\n",
    "print(\"gradient descent found\", len(rm_eq), \"equilibria:\", gd_eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76103ea7-18f0-4e24-872c-64d8e132c9d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'SymGame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m sym_game \u001b[38;5;241m=\u001b[39m game_from_appendix\n\u001b[0;32m----> 2\u001b[0m rm_eq \u001b[38;5;241m=\u001b[39m \u001b[43mNash_local_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43msym_game\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregret_matching\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m rd_eq \u001b[38;5;241m=\u001b[39m Nash_local_search(sym_game, replicator_dynamics, \u001b[38;5;241m10\u001b[39m, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, min_payoffs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m gd_eq \u001b[38;5;241m=\u001b[39m Nash_local_search(sym_game, gradient_descent, \u001b[38;5;241m10\u001b[39m, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36mNash_local_search\u001b[0;34m(sym_game, method, restarts, **search_kwds)\u001b[0m\n\u001b[1;32m     23\u001b[0m candidate \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(restarts):\n\u001b[0;32m---> 25\u001b[0m     prof \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_mixture\u001b[49m\u001b[43m(\u001b[49m\u001b[43msym_game\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     candidate\u001b[38;5;241m.\u001b[39mappend(method(sym_game, initial_profile \u001b[38;5;241m=\u001b[39m prof, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msearch_kwds))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(candidate)\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mrandom_mixture\u001b[0;34m(num_actions)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_mixture\u001b[39m(num_actions):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdirichlet(\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnum_actions\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'SymGame'"
     ]
    }
   ],
   "source": [
    "sym_game = game_from_appendix\n",
    "rm_eq = Nash_local_search(sym_game, regret_matching, 10, iterations=200, initial_weight=1)\n",
    "rd_eq = Nash_local_search(sym_game, replicator_dynamics, 10, iterations=200, min_payoffs=None)\n",
    "gd_eq = Nash_local_search(sym_game, gradient_descent, 10, iterations=200, step_size=0.01)\n",
    "print(\"regret matching found\", len(rm_eq), \"equilibria:\", rm_eq)\n",
    "print(\"replicator dynamics found\", len(rm_eq), \"equilibria:\", rd_eq)\n",
    "print(\"gradient descent found\", len(rm_eq), \"equilibria:\", gd_eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726ab32-ea28-4be0-9275-2105385abc95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
